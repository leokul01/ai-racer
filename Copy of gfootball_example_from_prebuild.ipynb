{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCWQ9yUT3W61"
   },
   "source": [
    "# Setup (should take < 100 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "shit = []\n",
    "for i in range(1, 11):\n",
    "    shit.append(pd.read_pickle('{}.pkl'.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['states', 'actions', 'values', 'masks', 'rewards', 'actions_probs', 'actions_onehot'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shit[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_r = shit[iters//2]['states']\n",
    "actions_r = shit[iters//2]['actions']\n",
    "values_r = shit[iters//2]['values']\n",
    "masks_r = shit[iters//2]['masks']\n",
    "rewards_r = shit[iters//2]['rewards']\n",
    "actions_probs_r = shit[iters//2]['actions_probs']\n",
    "actions_onehot_r = shit[iters//2]['actions_onehot']\n",
    "\n",
    "ACTIONS = OrderedDict({\n",
    "'ACC': np.array([0, 1, 0]),\n",
    "'IDLE': np.array([0, 0, 0]),\n",
    "'BR': np.array([0, 0, 0.4]),\n",
    "'LEFT': np.array([-1, 0, 0]),\n",
    "'RIGHT': np.array([1, 0, 0]),\n",
    "'LEFT_BR': np.array([-1, 0, 0.4]),\n",
    "'RIGHT_BR': np.array([1, 0, 0.4]),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGCu8kbl21Rx"
   },
   "source": [
    "# Now, you can run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4312,
     "status": "error",
     "timestamp": 1589033070644,
     "user": {
      "displayName": "Леонид Андреевич Куликов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2j0vjiR6YzLD8OrvDMjJnegosjQEcxIaTk3LUCw=s64",
      "userId": "11800375573557678329"
     },
     "user_tz": -180
    },
    "id": "vLg01fIo2lpV",
    "outputId": "df60af24-fa40-4f45-cf05-3b7b8dfefe75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda\\envs\\CarRace\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1039..1305 -> 266-tiles track\n",
      "retry to generate track (normal if there are not many instances of this message)\n",
      "Track generation: 1235..1548 -> 313-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CarRace\\lib\\site-packages\\keras_applications\\mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 96, 96, 3)         0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Model) multiple                  2257984   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1024)              11797504  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 14,062,663\n",
      "Trainable params: 11,804,679\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 96, 96, 3)         0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Model) multiple                  2257984   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1024)              11797504  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 14,056,513\n",
      "Trainable params: 11,798,529\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "itr: 0, action=[0.  0.  0.4], reward=6.310256410256411, q val=[[-0.78829056]]\n",
      "itr: 1, action=[-1.   0.   0.4], reward=-0.09999999999999964, q val=[[-0.86961085]]\n",
      "itr: 2, action=[0 0 0], reward=-0.09999999999999964, q val=[[-0.9806219]]\n",
      "itr: 3, action=[-1.   0.   0.4], reward=-0.09999999999999964, q val=[[-0.9578853]]\n",
      "itr: 4, action=[-1.   0.   0.4], reward=-0.09999999999999964, q val=[[-0.95725167]]\n",
      "itr: 5, action=[-1.   0.   0.4], reward=-0.09999999999999964, q val=[[-0.99251]]\n",
      "itr: 6, action=[-1.   0.   0.4], reward=-0.09999999999999964, q val=[[-0.9282075]]\n",
      "itr: 7, action=[-1.   0.   0.4], reward=-0.09999999999999964, q val=[[-0.97217405]]\n",
      "itr: 8, action=[-1.   0.   0.4], reward=-0.09999999999999964, q val=[[-0.9926213]]\n",
      "itr: 9, action=[-1.   0.   0.4], reward=-0.09999999999999964, q val=[[-0.9992628]]\n",
      "(10, 1, 1)\n",
      "(837, 1, 1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d373ce24f2d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0mq_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m     \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_advantages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m     actor_loss = model_actor.fit(\n\u001b[0;32m    269\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d373ce24f2d0>\u001b[0m in \u001b[0;36mget_advantages\u001b[1;34m(values, masks, rewards)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mgae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mgae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlmbda\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgae\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mreturns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgae\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "import random\n",
    "\n",
    "clipping_val = 0.2\n",
    "critic_discount = 0.5\n",
    "entropy_beta = 0.01\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "\n",
    "ACTIONS = OrderedDict({\n",
    "    'ACC': [0, 1, 0],\n",
    "    'IDLE': [0, 0, 0],\n",
    "    'BR': [0, 0, 1],\n",
    "#     'LEFT': [-1, 0, 0],\n",
    "#     'RIGHT': [1, 0, 0],\n",
    "    'LEFT_ACC': [-1, 0.5, 0],\n",
    "    'RIGHT_ACC': [1, 0.5, 0],\n",
    "#     'LEFT_BR': [-1, 0, 0.5],\n",
    "#     'RIGHT_BR': [1, 0, 0.5]\n",
    "})\n",
    "\n",
    "for i in range(100):\n",
    "    ACTIONS['{}'.format(i)] = [random.uniform(-1,1), random.random(), random.random()/2] \n",
    "\n",
    "def get_advantages(values, masks, rewards):\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "        gae = delta + gamma * lmbda * masks[i] * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    adv = np.array(returns) - values[:-1]\n",
    "    std_adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "    return returns, std_adv\n",
    "\n",
    "\n",
    "def ppo_loss_print(oldpolicy_probs, advantages, rewards, values):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.Print(y_true, [y_true], 'y_true: ')\n",
    "        y_pred = tf.Print(y_pred, [y_pred], 'y_pred: ')\n",
    "        newpolicy_probs = y_pred\n",
    "        # newpolicy_probs = y_true * y_pred\n",
    "        newpolicy_probs = tf.Print(newpolicy_probs, [newpolicy_probs], 'new policy probs: ')\n",
    "\n",
    "        ratio = K.exp(K.log(newpolicy_probs + 1e-10) - K.log(oldpolicy_probs + 1e-10))\n",
    "        ratio = tf.Print(ratio, [ratio], 'ratio: ')\n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - clipping_val, max_value=1 + clipping_val) * advantages\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "        actor_loss = tf.Print(actor_loss, [actor_loss], 'actor_loss: ')\n",
    "        critic_loss = K.mean(K.square(rewards - values))\n",
    "        critic_loss = tf.Print(critic_loss, [critic_loss], 'critic_loss: ')\n",
    "        term_a = critic_discount * critic_loss\n",
    "        term_a = tf.Print(term_a, [term_a], 'term_a: ')\n",
    "        term_b_2 = K.log(newpolicy_probs + 1e-10)\n",
    "        term_b_2 = tf.Print(term_b_2, [term_b_2], 'term_b_2: ')\n",
    "        term_b = entropy_beta * K.mean(-(newpolicy_probs * term_b_2))\n",
    "        term_b = tf.Print(term_b, [term_b], 'term_b: ')\n",
    "        total_loss = term_a + actor_loss - term_b\n",
    "        total_loss = tf.Print(total_loss, [total_loss], 'total_loss: ')\n",
    "        return total_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def ppo_loss(oldpolicy_probs, advantages, rewards, values):\n",
    "    def loss(y_true, y_pred):\n",
    "        newpolicy_probs = y_pred\n",
    "        ratio = K.exp(K.log(newpolicy_probs + 1e-10) - K.log(oldpolicy_probs + 1e-10))\n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - clipping_val, max_value=1 + clipping_val) * advantages\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "        critic_loss = K.mean(K.square(rewards - values))\n",
    "        total_loss = critic_discount * critic_loss + actor_loss - entropy_beta * K.mean(\n",
    "            -(newpolicy_probs * K.log(newpolicy_probs + 1e-10)))\n",
    "        return total_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_model_actor_image(input_dims, output_dims):\n",
    "    state_input = Input(shape=input_dims)\n",
    "    oldpolicy_probs = Input(shape=(1, output_dims,))\n",
    "    advantages = Input(shape=(1, 1,))\n",
    "    rewards = Input(shape=(1, 1,))\n",
    "    values = Input(shape=(1, 1,))\n",
    "\n",
    "    feature_extractor = MobileNetV2(include_top=False, weights='imagenet')\n",
    "\n",
    "    for layer in feature_extractor.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Classification block\n",
    "    x = Flatten(name='flatten')(feature_extractor(state_input))\n",
    "    x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "    out_actions = Dense(n_actions, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model(inputs=[state_input, oldpolicy_probs, advantages, rewards, values],\n",
    "                  outputs=[out_actions])\n",
    "    model.compile(optimizer=Adam(lr=1e-3), loss=[ppo_loss(\n",
    "        oldpolicy_probs=oldpolicy_probs,\n",
    "        advantages=advantages,\n",
    "        rewards=rewards,\n",
    "        values=values)])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_critic_image(input_dims):\n",
    "    state_input = Input(shape=input_dims)\n",
    "\n",
    "    feature_extractor = MobileNetV2(include_top=False, weights='imagenet')\n",
    "\n",
    "    for layer in feature_extractor.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Classification block\n",
    "    x = Flatten(name='flatten')(feature_extractor(state_input))\n",
    "    x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "    out_actions = Dense(1, activation='tanh')(x)\n",
    "\n",
    "    model = Model(inputs=[state_input], outputs=[out_actions])\n",
    "    model.compile(optimizer=Adam(lr=1e-3), loss='mse')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "# def test_reward():\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "#     print('testing...')\n",
    "#     limit = 0\n",
    "#     while not done:\n",
    "#         state_input = K.expand_dims(state, 0)\n",
    "#         action_probs = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "#         action = np.argmax(action_probs)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#         state = next_state\n",
    "#         total_reward += reward\n",
    "#         limit += 1\n",
    "#         if limit > 20:\n",
    "#             break\n",
    "#     return total_reward\n",
    "\n",
    "def test_reward():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    print('testing...')\n",
    "    limit = 0\n",
    "    while not done:\n",
    "        state_input = K.expand_dims(state, 0)\n",
    "        action_probs = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "        action_ind = np.argmax(action_probs)\n",
    "        action = list(ACTIONS.values())[action_ind]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        limit += 1\n",
    "        if limit > 200:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def one_hot_encoding(probs):\n",
    "    one_hot = np.zeros_like(probs)\n",
    "    one_hot[:, np.argmax(probs, axis=1)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "env = gym.make(\"CarRacing-v0\")\n",
    "state = env.reset()\n",
    "state_dims = env.observation_space.shape\n",
    "n_actions = len(ACTIONS)\n",
    "\n",
    "dummy_n = np.random.rand(1, 1, n_actions)\n",
    "dummy_1 = np.zeros((1, 1, 1))\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='./logs')\n",
    "\n",
    "model_actor = get_model_actor_image(input_dims=state_dims, output_dims=n_actions)\n",
    "model_critic = get_model_critic_image(input_dims=state_dims)\n",
    "\n",
    "ppo_steps = 1024\n",
    "target_reached = False\n",
    "best_reward = 0\n",
    "iters = 0\n",
    "max_iters = 25\n",
    "while not target_reached and iters < max_iters:\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    masks = []\n",
    "    rewards = []\n",
    "    actions_probs = []\n",
    "    actions_onehot = []\n",
    "    state_input = None\n",
    "\n",
    "    for itr in range(ppo_steps):\n",
    "        state_input = K.expand_dims(state, 0)\n",
    "        action_dist = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "        if iters == 0:\n",
    "            action_dist = np.array([1/len(ACTIONS) for i in range(len(ACTIONS))]).reshape(1,len(ACTIONS))\n",
    "        q_value = model_critic.predict([state_input], steps=1)\n",
    "        action_ind = np.random.choice(n_actions, p=action_dist[0, :])\n",
    "        action_onehot = np.zeros(n_actions)\n",
    "        action_onehot[action_ind] = 1\n",
    "        action = list(ACTIONS.values())[action_ind]\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        print('itr: ' + str(itr) + ', action=' + str(action) + ', reward=' + str(reward) + ', q val=' + str(q_value))\n",
    "        mask = not done\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        actions_onehot.append(action_onehot)\n",
    "        values.append(q_value)\n",
    "        masks.append(mask)\n",
    "        rewards.append(reward)\n",
    "        actions_probs.append(action_dist)\n",
    "\n",
    "        state = observation\n",
    "        if done:\n",
    "            env.reset()\n",
    "\n",
    "    q_value = model_critic.predict(state_input, steps=1)\n",
    "    values.append(q_value)\n",
    "    returns, advantages = get_advantages(values, masks, rewards)\n",
    "    actor_loss = model_actor.fit(\n",
    "        [states, actions_probs, advantages, np.reshape(rewards, newshape=(-1, 1, 1)), values[:-1]],\n",
    "        [(np.reshape(actions_onehot, newshape=(-1, n_actions)))], verbose=True, shuffle=True, epochs=8,\n",
    "        callbacks=None)\n",
    "    critic_loss = model_critic.fit([states], [np.reshape(returns, newshape=(-1, 1))], shuffle=True, epochs=8,\n",
    "                                   verbose=True, callbacks=None)\n",
    "\n",
    "    avg_reward = np.mean([test_reward() for _ in range(1)])\n",
    "    print('total test reward=' + str(avg_reward))\n",
    "    if avg_reward > best_reward:\n",
    "        print('best reward=' + str(avg_reward))\n",
    "        model_actor.save('model_actor_{}_{}.hdf5'.format(iters, avg_reward))\n",
    "        model_critic.save('model_critic_{}_{}.hdf5'.format(iters, avg_reward))\n",
    "        best_reward = avg_reward\n",
    "    if best_reward > 100 or iters > max_iters:\n",
    "        target_reached = True\n",
    "    iters += 1\n",
    "    env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.reshape(shit[iters//2]['rewards'], newshape=(-1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SswI4PYZYv1k"
   },
   "outputs": [],
   "source": [
    "np.reshape(shit[iters//2]['actions_onehot'], newshape=(-1, n_actions)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values[:-1]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of gfootball_example_from_prebuild.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/google-research/football/blob/master/gfootball/colabs/gfootball_example_from_prebuild.ipynb",
     "timestamp": 1589027611472
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
